{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632bc5c7-3383-479b-bc2b-053037e66c18",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a66df1e2-3718-4cfe-83de-61750940bbe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.56.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (3.4.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (67.7.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (0.40.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torch_optimizer in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch_optimizer) (2.0.0)\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch_optimizer) (0.1.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard\n",
    "!pip install torch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a543005-cabc-41ad-8649-287c81ca6ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "# from skimage import io,transform,img_as_ubyte\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_optimizer import Lookahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f05bc-d0b6-4be3-9952-98b92c3fa7ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77b8ced9-17b3-4796-a75b-417ec9e73c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ZooplanktonDataset(Dataset):\n",
    "    def __init__(self, csv, cropped_image_folder, transform=None):\n",
    "        self.annotations = pd.read_csv(csv)\n",
    "        self.cropped_image_folder = cropped_image_folder\n",
    "        self.transform = transform\n",
    "        self.labels = np.array(self.annotations.iloc[:, 1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.annotations.iloc[index, 0]\n",
    "        cropped_img_path = os.path.join(self.cropped_image_folder, img_name)\n",
    "        \n",
    "        image = Image.open(cropped_img_path).convert('RGB')\n",
    "        \n",
    "        y_label = self.annotations.iloc[index, 1]\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, y_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8af52db-116d-4f52-bae8-10ad16f0a976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Downsampleing Majority class\n",
    "\n",
    "# from sklearn.utils import resample\n",
    "# class ZooplanktonDataset(Dataset):\n",
    "#     def __init__(self, csv, cropped_image_folder, transform=None):\n",
    "#         self.labels = pd.read_csv(csv)  \n",
    "#         self.cropped_image_folder = cropped_image_folder\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # Separate samples for classes 0 and 1\n",
    "#         class_0_samples = self.labels[self.labels['Label'] == 0]\n",
    "#         class_1_samples = self.labels[self.labels['Label'] == 1]\n",
    "        \n",
    "#         # Downsample class 0 to match the size of class 1\n",
    "#         class_0_downsampled = resample(class_0_samples,\n",
    "#                                        replace=False,  # Without replacement\n",
    "#                                        n_samples=len(class_1_samples),\n",
    "#                                        random_state=42)  # For reproducibility\n",
    "        \n",
    "#         # Concatenate downsampled class 0 and class 1 samples\n",
    "#         self.labels = pd.concat([class_0_downsampled, class_1_samples])\n",
    "#         self.labels = self.labels.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         img_name = self.labels.iloc[index, 0]  # Change from self.annotations to self.labels\n",
    "#         cropped_img_path = os.path.join(self.cropped_image_folder, img_name)\n",
    "        \n",
    "#         image = Image.open(cropped_img_path).convert('RGB')\n",
    "        \n",
    "#         y_label = self.labels.iloc[index, 1]  # Change from self.annotations to self.labels\n",
    "            \n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         return image, y_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a273a85c-fe78-4ae8-8198-d2570efb8c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Upsampling Minority class\n",
    "\n",
    "# def custom_collate(batch):\n",
    "#     images, labels = zip(*batch)\n",
    "#     images = torch.stack([transforms.ToTensor()(img) for img in images], dim=0)\n",
    "#     labels = torch.tensor(labels)\n",
    "#     return images, labels\n",
    "\n",
    "# from sklearn.utils import resample\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# from torch.utils.data import Dataset\n",
    "# from torchvision import transforms\n",
    "\n",
    "# class ZooplanktonDataset(Dataset):\n",
    "#     def __init__(self, csv, cropped_image_folder, transform=None):\n",
    "#         self.labels = pd.read_csv(csv)  \n",
    "#         self.cropped_image_folder = cropped_image_folder\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         class_0_samples = self.labels[self.labels['Label'] == 0]\n",
    "#         class_1_samples = self.labels[self.labels['Label'] == 1]\n",
    "        \n",
    "#         # Upsample class 1 to match the size of class 0\n",
    "#         class_1_upsampled = resample(class_1_samples,\n",
    "#                                      replace=True,  # With replacement for upsampling\n",
    "#                                      n_samples=len(class_0_samples),\n",
    "#                                      random_state=42)\n",
    "        \n",
    "#         self.labels = pd.concat([class_0_samples, class_1_upsampled])\n",
    "#         self.labels = self.labels.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         img_name = self.labels.iloc[index, 0]\n",
    "#         cropped_img_path = os.path.join(self.cropped_image_folder, img_name)\n",
    "        \n",
    "#         image = Image.open(cropped_img_path).convert('RGB')\n",
    "        \n",
    "#         y_label = self.labels.iloc[index, 1]\n",
    "            \n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         return image, y_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76534ffa-dbb5-49ae-98f0-579338d62172",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train and Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c23216f-6606-45c2-a314-6925f62aa8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# data_transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.RandomRotation(10),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# random_transforms = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomRotation(degrees=20)\n",
    "# ])\n",
    "\n",
    "# dataset = ZooplanktonDataset(csv='/home/ec2-user/SageMaker/All_csv_combined/annotations.csv',cropped_image_folder='/home/ec2-user/SageMaker/dataset/uncropped+predictions1',\n",
    "#                             transform=data_transform)\n",
    "\n",
    "dataset = ZooplanktonDataset(csv='/home/ec2-user/SageMaker/All_csv_combined/annotations.csv',\n",
    "                             cropped_image_folder='/home/ec2-user/SageMaker/dataset/uncropped+predictions1',\n",
    "                             transform=data_transform)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# # Splitting the indices into train and test sets with a stratified split\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(dataset)), test_size=0.2, stratify=dataset.labels\n",
    ")\n",
    "\n",
    "# train_indices, test_indices = train_test_split(\n",
    "#     np.arange(len(dataset)), test_size=0.2\n",
    "# )\n",
    "\n",
    "# Create subset datasets using the train and test indices\n",
    "train_set = Subset(dataset, train_indices)\n",
    "test_set = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663f213-8f71-46c3-9463-446d4a663454",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aff2c3e4-9ce5-41eb-b707-ed120b8db684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model_v2(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(Model_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Adjusting the fully connected layers to fit the spatial dimensions\n",
    "        # of the input image (256x256)\n",
    "        self.fc1 = nn.Linear(64 * 30 * 30, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)  # Output layer for binary classification (1 neuron for probability)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for probability output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(F.relu(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 64 * 30 * 30)  # Adjusted view shape\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.sigmoid(self.fc3(x))  # Use sigmoid activation for probability output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5086f73-2884-44d2-a164-960240c793c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model_v2(nn.Module):\n",
    "#     def __init__(self, dropout):\n",
    "#         super(Model_v2, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "#         self.pool1 = nn.AdaptiveMaxPool2d((3, 3))\n",
    "#         self.pool2 = nn.AdaptiveMaxPool2d((3, 3))\n",
    "#         self.pool3 = nn.AdaptiveMaxPool2d((2, 2))\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # Adjusting the fully connected layers to fit the spatial dimensions\n",
    "#         # of the input image (256x256)\n",
    "#         self.fc1 = nn.Linear(64 * 2 * 2, 256)  # Adjusted input size\n",
    "#         self.fc2 = nn.Linear(256, 84)\n",
    "#         self.fc3 = nn.Linear(84, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.dropout(F.relu(self.conv1(x)))\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.dropout(F.relu(self.conv2(x)))\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.dropout(F.relu(self.conv3(x)))\n",
    "#         x = self.pool3(x)\n",
    "        \n",
    "#         # Flatten the tensor before fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         x = self.dropout(F.relu(self.fc1(x)))\n",
    "#         x = self.dropout(F.relu(self.fc2(x)))\n",
    "#         x = self.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb8ad3-2aec-4f7d-8ddd-2a229c5af1a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loss curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "064031cd-6a36-479e-9e02-ffa0eac97b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossCurve:\n",
    "    def __init__(self):\n",
    "        self\n",
    "\n",
    "    def PlotCurve(loss_values,epochs):\n",
    "        plt.plot(range(epochs), loss_values, 'blue')\n",
    "        plt.title('Loss decay')\n",
    "        plt.xlabel('number of epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bd25c-d5fe-45e2-a39b-885db688590d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a592bbe-76ae-4b74-9f7e-f18626f3a04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, loss_criterion, threshold, train_data_loader, test_data_loader,\n",
    "                 optimizer, epochs, with_cuda, use_scheduler, scheduler, save_dir, tensorboard_dir,\n",
    "                 lr=0.001, print_freq=1):\n",
    "        self.device = torch.device(\"cuda:0\" if with_cuda else \"cpu\")\n",
    "        self.model = model\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.with_cuda = with_cuda\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.lr = lr\n",
    "        self.save_dir = save_dir\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.use_scheduler = use_scheduler\n",
    "        self.scheduler = scheduler\n",
    "        self.step = 0\n",
    "        self.print_freq = print_freq\n",
    "        self.start_time = datetime.now().strftime('%b-%d_%H-%M')\n",
    "        self.save_freq = 1\n",
    "        self.accuracy = 0\n",
    "        self.threshold = threshold\n",
    "        if not os.path.exists(self.tensorboard_dir):\n",
    "            os.makedirs(self.tensorboard_dir)\n",
    "        self.writer = SummaryWriter(log_dir=self.tensorboard_dir)\n",
    "        \n",
    "    def _train_epoch(self):\n",
    "        loss_values = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0\n",
    "            last_step = self.step - 1\n",
    "            last_time = time.time()\n",
    "            batch_counter = 0\n",
    "            accuracy = 0\n",
    "            loop = tqdm(enumerate(self.train_data_loader), total=len(self.train_data_loader), leave=True)\n",
    "            for batch_idx, (images, labels) in loop:\n",
    "                if self.with_cuda:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.model.train()\n",
    "                batch_counter += 1\n",
    "                # forward pass\n",
    "                outputs = self.model(images)\n",
    "                labels = labels.unsqueeze(1).float()\n",
    "                # finding loss\n",
    "                loss = self.loss_criterion(outputs, labels)\n",
    "                # backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.use_scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                # evaluation of model on test data\n",
    "                if batch_counter % 1000 == 0 or batch_counter == len(self.train_data_loader):\n",
    "                    accuracy = self._valid_epoch(self.model, epoch, self.test_data_loader)\n",
    "                \n",
    "                is_best = False\n",
    "                if accuracy > self.accuracy:\n",
    "                    is_best = True\n",
    "                self.accuracy = accuracy\n",
    "                \n",
    "                if self.step % self.print_freq == self.print_freq - 1:\n",
    "                    used_time = time.time() - last_time\n",
    "                    step_num = self.step - last_step\n",
    "                    speed = self.train_data_loader.batch_size * step_num / used_time\n",
    "                    batch_loss = loss.item()\n",
    "                    loop.set_description(f\"Epoch[{epoch + 1}/{self.epochs}]\")\n",
    "                    loop.set_postfix(\n",
    "                        lr=self.scheduler.get_last_lr(),\n",
    "                        batch_loss=batch_loss,\n",
    "                        global_loss=running_loss,\n",
    "                        speed=speed,\n",
    "                        accuracy=self.accuracy,\n",
    "                        device=self.device\n",
    "                    )\n",
    "                    global_loss = 0.0\n",
    "                    last_step = self.step\n",
    "                    last_time = time.time()\n",
    "                self.step += 1\n",
    "\n",
    "            if epoch % self.save_freq == 0:\n",
    "                self._save_checkpoint(epoch, is_best)\n",
    "            \n",
    "            self.writer.add_scalar('Loss', running_loss / len(self.train_data_loader), global_step=self.step)\n",
    "            loss_values.append(running_loss / len(self.train_data_loader))\n",
    "        \n",
    "        print(\"Train Accuracy:\", self._valid_epoch(self.model, self.epochs, self.train_data_loader))\n",
    "        print(\"Validation Accuracy:\", self._valid_epoch(self.model, self.epochs, self.test_data_loader))\n",
    "        \n",
    "        # Find the best threshold and its corresponding accuracy\n",
    "        best_threshold, best_accuracy = self._find_best_threshold()\n",
    "        print(\"Best Threshold:\", best_threshold)\n",
    "        print(\"Best Validation Accuracy:\", best_accuracy)\n",
    "        \n",
    "        # Plotting Loss Curve\n",
    "        self.writer.add_scalar('Best_Validation_Accuracy', best_accuracy, global_step=self.epochs)\n",
    "        LossCurve.PlotCurve(loss_values, self.epochs)\n",
    "        \n",
    "        # logging images to Tensorboard\n",
    "        self.log_images_after_training()\n",
    "        \n",
    "        # closing SummaryWriter Object\n",
    "        self.writer.close()\n",
    "\n",
    "    def _valid_epoch(self, model, epoch, test_loader, threshold=None):\n",
    "        model.eval()\n",
    "        if threshold is None:\n",
    "            threshold = self.threshold  # Use the current threshold if not provided\n",
    "        with torch.no_grad():\n",
    "            correct_samples = 0\n",
    "            total_samples = 0\n",
    "            for images, labels in test_loader:\n",
    "                if self.with_cuda:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                pred_probs = model(images)\n",
    "                pred_labels = (pred_probs > threshold).float()\n",
    "                total_samples += labels.size(0)\n",
    "                correct_samples += (pred_labels == labels.view(-1, 1).float()).sum().item()\n",
    "            accuracy = (correct_samples / total_samples) * 100\n",
    "            self.writer.add_scalar('Accuracy', accuracy, global_step=epoch)\n",
    "            return accuracy\n",
    "\n",
    "    def _find_best_threshold(self, num_thresholds=100):\n",
    "        # Evaluate the model on the validation set for different thresholds\n",
    "        thresholds = np.linspace(0.1, 1, num_thresholds)\n",
    "        false_negatives = []\n",
    "        for threshold in thresholds:\n",
    "            false_negative = self._calculate_false_negatives(self.model, self.test_data_loader, threshold)\n",
    "            false_negatives.append(false_negative)\n",
    "\n",
    "        # Find the best threshold that gives the least false negatives\n",
    "        best_idx = np.argmin(false_negatives)\n",
    "        best_threshold = thresholds[best_idx]\n",
    "        best_false_negative = false_negatives[best_idx]\n",
    "\n",
    "        # Update the threshold in the class\n",
    "        self.threshold = best_threshold\n",
    "\n",
    "        return best_threshold, best_false_negative\n",
    "\n",
    "    def _calculate_false_negatives(self, model, test_loader, threshold):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            false_negatives = 0\n",
    "            total_positives = 0\n",
    "            for images, labels in test_loader:\n",
    "                if self.with_cuda:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                pred_probs = model(images)\n",
    "                pred_labels = (pred_probs > threshold).float()\n",
    "\n",
    "                # Count the number of false negatives (predicted as negative but actually positive)\n",
    "                false_negatives += ((pred_labels == 0) & (labels == 1)).sum().item()\n",
    "                total_positives += (labels == 1).sum().item()\n",
    "\n",
    "            false_negative_rate = false_negatives / max(total_positives, 1)  # Avoid division by zero\n",
    "            return false_negative_rate\n",
    "\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, is_best):\n",
    "        arch = type(self.model).__name__\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'arch': arch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'accuracy': self.accuracy,\n",
    "            'step': self.step + 1,\n",
    "            'start_time': self.start_time}\n",
    "        filename = os.path.join(\n",
    "            self.save_dir + 'checkpoint_{}_epoch{:02d}_acc_{:.5f}.pth.tar'.format(\n",
    "                arch, epoch, self.accuracy))\n",
    "        print(\"Saving checkpoint: {} ...\".format(filename))\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "        torch.save(state, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(\n",
    "                filename, os.path.join(self.save_dir, '{}_binary_best.pth.tar'.format(arch)))\n",
    "        return filename\n",
    "\n",
    "    def log_images_after_training(self):\n",
    "        arch = type(self.model).__name__\n",
    "        checkpoint = torch.load('/home/ec2-user/SageMaker/checkpoints/uncropped/{}_binary_best.pth.tar'.format(arch))\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        self.model.eval()\n",
    "        images = []\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        for batch_images, batch_labels in self.test_data_loader:\n",
    "            with torch.no_grad():\n",
    "                if self.with_cuda:\n",
    "                    batch_images, batch_labels = batch_images.to(self.device), batch_labels.to(self.device)\n",
    "\n",
    "                # Forward pass to obtain predicted labels for each item in the batch\n",
    "                batch_outputs = self.model(batch_images)\n",
    "                batch_predicted_labels = (batch_outputs >= self.threshold).float()  # Convert probabilities to binary predictions (0 or 1)\n",
    "            # Iterate over each item in the batch\n",
    "            for i in range(batch_images.size(0)):\n",
    "                image = vutils.make_grid(batch_images[i], nrow=1, padding=10, normalize=True) \n",
    "                true_label = batch_labels[i]\n",
    "                predicted_label = batch_predicted_labels[i]\n",
    "\n",
    "                images.append(image)\n",
    "                true_labels.append(true_label)\n",
    "                predicted_labels.append(predicted_label)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            true_label = true_labels[i]\n",
    "            predicted_label = predicted_labels[i]\n",
    "\n",
    "            # Set the image name as the predicted and true labels\n",
    "            image_name = f\"Image-{i}-True-{true_label}-Predicted-{predicted_label}\"\n",
    "\n",
    "            # Log the image on TensorBoard with the image name\n",
    "            self.writer.add_image(image_name, image.cpu(), global_step=i)\n",
    "\n",
    "        #creating confusion matrix\n",
    "        true_labels = torch.tensor(true_labels)\n",
    "        predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.colorbar()\n",
    "\n",
    "        # Set the axis labels and ticks\n",
    "        class_names = ['Zooplankton', 'Marine-Snow']\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        # Add labels to each cell of the confusion matrix\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        # Save the confusion matrix figure to TensorBoard\n",
    "        self.writer.add_figure('Confusion Matrix', fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2f1e9-e45c-4e62-8a35-7330b88283ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9087ec74-10c8-4b63-9d89-acf6159d26f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "    #model\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model_v2(0.2)\n",
    "    model.to(device)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    # Base Optimizer\n",
    "    base_optimizer = optim.RAdam(\n",
    "        params=parameters,\n",
    "        lr=0.001,\n",
    "        betas=(0.8, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=1e-5)\n",
    "\n",
    "    #Lookahead optimizer\n",
    "    optimizer = Lookahead(base_optimizer, k=10, alpha=0.5)  \n",
    "    #scheduler\n",
    "    lr_warm_up_num = 100\n",
    "    cr = 1.0 / math.log(lr_warm_up_num)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda ee: cr * math.log(ee + 1)\n",
    "        if ee < lr_warm_up_num else 1)\n",
    "    data = pd.read_csv('/home/ec2-user/SageMaker/All_csv_combined/Test/annotations.csv')\n",
    "    label_counts = data.iloc[:, 1].value_counts()\n",
    "    loss_ratio = label_counts[0] / label_counts[1]\n",
    "    class_weights = torch.Tensor([1,1]).to(device)  # Inverse ratio of class frequencies\n",
    "    print(\"Class Weights:\", class_weights)\n",
    "\n",
    "    # Define the BCE loss with class weights\n",
    "    loss_criterion = torch.nn.BCELoss(class_weights[1])\n",
    "    #creating object for tensorboard_dir\n",
    "    tensorboard_dir = '/home/ec2-user/SageMaker/runs/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf15377-d31a-413c-be7e-42be733190d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trainer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86016839-5292-4fd5-8055-6520441a1397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "   trainer = Trainer(\n",
    "        model, loss_criterion,\n",
    "        train_data_loader=train_loader,\n",
    "        threshold = 0.8,\n",
    "        test_data_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=30,\n",
    "        with_cuda=True,\n",
    "        use_scheduler=True,\n",
    "        scheduler=scheduler,\n",
    "        save_dir='/home/ec2-user/SageMaker/checkpoints/uncropped/',\n",
    "        tensorboard_dir=tensorboard_dir,\n",
    "        lr=0.001)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f57ca-0722-4460-a193-997ba6ccb10d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e2fea83-1261-44a5-a96a-2e644d6c073e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[1/30]: 100%|██████████| 855/855 [01:17<00:00, 10.99it/s, accuracy=85.6, batch_loss=0.925, device=cuda:0, global_loss=372, lr=[0.001], speed=1.36]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch00_acc_85.57214.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[2/30]: 100%|██████████| 855/855 [01:04<00:00, 13.24it/s, accuracy=85.5, batch_loss=0.431, device=cuda:0, global_loss=343, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch01_acc_85.54287.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[3/30]: 100%|██████████| 855/855 [01:03<00:00, 13.56it/s, accuracy=85.6, batch_loss=0.777, device=cuda:0, global_loss=318, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch02_acc_85.57214.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[4/30]: 100%|██████████| 855/855 [01:05<00:00, 13.09it/s, accuracy=85.6, batch_loss=0.804, device=cuda:0, global_loss=298, lr=[0.001], speed=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch03_acc_85.57214.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[5/30]: 100%|██████████| 855/855 [01:03<00:00, 13.47it/s, accuracy=85.6, batch_loss=0.0643, device=cuda:0, global_loss=271, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch04_acc_85.63067.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[6/30]: 100%|██████████| 855/855 [01:03<00:00, 13.52it/s, accuracy=85.6, batch_loss=0.0686, device=cuda:0, global_loss=281, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch05_acc_85.60140.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[7/30]: 100%|██████████| 855/855 [01:04<00:00, 13.24it/s, accuracy=85.7, batch_loss=0.17, device=cuda:0, global_loss=242, lr=[0.001], speed=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch06_acc_85.71847.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[8/30]: 100%|██████████| 855/855 [01:09<00:00, 12.32it/s, accuracy=86.2, batch_loss=0.00182, device=cuda:0, global_loss=212, lr=[0.001], speed=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch07_acc_86.21598.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[9/30]: 100%|██████████| 855/855 [01:03<00:00, 13.37it/s, accuracy=86.2, batch_loss=0.0525, device=cuda:0, global_loss=191, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch08_acc_86.24524.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[10/30]: 100%|██████████| 855/855 [01:03<00:00, 13.39it/s, accuracy=86.9, batch_loss=0.1, device=cuda:0, global_loss=174, lr=[0.001], speed=1.81] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch09_acc_86.91835.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[11/30]: 100%|██████████| 855/855 [01:03<00:00, 13.48it/s, accuracy=86.6, batch_loss=0.0182, device=cuda:0, global_loss=146, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch10_acc_86.59643.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[12/30]: 100%|██████████| 855/855 [01:03<00:00, 13.45it/s, accuracy=86.9, batch_loss=0.406, device=cuda:0, global_loss=135, lr=[0.001], speed=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch11_acc_86.88908.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[13/30]: 100%|██████████| 855/855 [01:04<00:00, 13.28it/s, accuracy=87.6, batch_loss=0.000348, device=cuda:0, global_loss=107, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch12_acc_87.64999.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[14/30]: 100%|██████████| 855/855 [01:03<00:00, 13.53it/s, accuracy=87.4, batch_loss=0.0116, device=cuda:0, global_loss=84.6, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch13_acc_87.35733.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[15/30]: 100%|██████████| 855/855 [01:04<00:00, 13.21it/s, accuracy=87.6, batch_loss=0.0537, device=cuda:0, global_loss=81.5, lr=[0.001], speed=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch14_acc_87.62072.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[16/30]: 100%|██████████| 855/855 [01:03<00:00, 13.47it/s, accuracy=88.2, batch_loss=0.00746, device=cuda:0, global_loss=65.7, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch15_acc_88.20603.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[17/30]: 100%|██████████| 855/855 [01:03<00:00, 13.54it/s, accuracy=88.2, batch_loss=0.0046, device=cuda:0, global_loss=62.3, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch16_acc_88.23529.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[18/30]: 100%|██████████| 855/855 [01:04<00:00, 13.30it/s, accuracy=86.9, batch_loss=0.00225, device=cuda:0, global_loss=69, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch17_acc_86.88908.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[19/30]: 100%|██████████| 855/855 [01:02<00:00, 13.59it/s, accuracy=87.9, batch_loss=0.0106, device=cuda:0, global_loss=66, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch18_acc_87.91337.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[20/30]: 100%|██████████| 855/855 [01:03<00:00, 13.54it/s, accuracy=88.2, batch_loss=1.49, device=cuda:0, global_loss=45.7, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch19_acc_88.20603.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[21/30]: 100%|██████████| 855/855 [01:03<00:00, 13.46it/s, accuracy=88.1, batch_loss=0.00466, device=cuda:0, global_loss=46.7, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch20_acc_88.05970.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[22/30]: 100%|██████████| 855/855 [01:03<00:00, 13.57it/s, accuracy=88.2, batch_loss=0.0088, device=cuda:0, global_loss=41.9, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch21_acc_88.17676.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[23/30]: 100%|██████████| 855/855 [01:03<00:00, 13.54it/s, accuracy=87.8, batch_loss=4.83e-6, device=cuda:0, global_loss=48.3, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch22_acc_87.76705.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[24/30]: 100%|██████████| 855/855 [01:03<00:00, 13.57it/s, accuracy=87.7, batch_loss=0.158, device=cuda:0, global_loss=40.4, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch23_acc_87.73778.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[25/30]: 100%|██████████| 855/855 [01:03<00:00, 13.52it/s, accuracy=88.2, batch_loss=0.000174, device=cuda:0, global_loss=31.3, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch24_acc_88.23529.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[26/30]: 100%|██████████| 855/855 [01:03<00:00, 13.49it/s, accuracy=88.4, batch_loss=6.98e-5, device=cuda:0, global_loss=30.8, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch25_acc_88.44015.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[27/30]: 100%|██████████| 855/855 [01:03<00:00, 13.40it/s, accuracy=88.3, batch_loss=1.92e-8, device=cuda:0, global_loss=28.8, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch26_acc_88.26456.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[28/30]: 100%|██████████| 855/855 [01:03<00:00, 13.53it/s, accuracy=87.4, batch_loss=2.36e-9, device=cuda:0, global_loss=30.2, lr=[0.001], speed=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch27_acc_87.38660.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[29/30]: 100%|██████████| 855/855 [01:03<00:00, 13.36it/s, accuracy=88, batch_loss=0.0571, device=cuda:0, global_loss=26.6, lr=[0.001], speed=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch28_acc_87.97191.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[30/30]: 100%|██████████| 855/855 [01:03<00:00, 13.52it/s, accuracy=88.5, batch_loss=0.000388, device=cuda:0, global_loss=30.1, lr=[0.001], speed=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch29_acc_88.46942.pth.tar ...\n",
      "Train Accuracy: 99.04141665447095\n",
      "Validation Accuracy: 88.46941761779338\n",
      "Best Threshold: 0.1\n",
      "Best Validation Accuracy: 12.4434250764526\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer._train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "569d735d-e88e-49c1-ad63-9bee895ea005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# hyperparameter_grid = {\n",
    "#     'model': [Model_v2], # adding more layers # adding more maxpool and make \n",
    "#     'lr': [0.001], # imp\n",
    "#     'weight_decay': [1e-7],\n",
    "#     'dropout': [0.2], #imp\n",
    "#     'loss_weight': [0.3,0.5,1.0,1.5,2.0,5.0],\n",
    "#     'batch_size': [16] # most imp\n",
    "# }\n",
    "\n",
    "# # Create a list of all possible combinations of hyperparameters\n",
    "# hyperparameter_combinations = list(itertools.product(*hyperparameter_grid.values()))\n",
    "\n",
    "# # Define a function for training and evaluating the model with a given set of hyperparameters\n",
    "# def train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size):\n",
    "#     # Create the model\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model_class(dropout)\n",
    "#     model.to(device)\n",
    "#     parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    \n",
    "#     # Create the optimizer\n",
    "#     optimizer = optim.Adam(\n",
    "#         params=parameters,\n",
    "#         lr=lr,\n",
    "#         betas=(0.8, 0.999),\n",
    "#         eps=1e-8,\n",
    "#         weight_decay=weight_decay)\n",
    "#     tensorboard_dir = '/home/ec2-user/SageMaker/runs/'\n",
    "#     # Create the scheduler\n",
    "#     lr_warm_up_num = 100\n",
    "#     cr = 1.0 / math.log(lr_warm_up_num)\n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(\n",
    "#         optimizer,\n",
    "#         lr_lambda=lambda ee: cr * math.log(ee + 1)\n",
    "#         if ee < lr_warm_up_num else 1)\n",
    "    \n",
    "#     # Create the loss criterion\n",
    "#     data = pd.read_csv('dataset/uncropped/annotations.csv')\n",
    "#     label_counts = data.iloc[:, 1].value_counts()\n",
    "#     loss_ratio = label_counts[0] / label_counts[1]\n",
    "#     weight = torch.Tensor([1,loss_ratio*loss_weight]).to(device)\n",
    "#     loss_criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    \n",
    "#     # Load the data with the specified batch size\n",
    "#     dataset_length = len(dataset)\n",
    "#     train_size = int(0.7 * dataset_length)\n",
    "#     test_size = dataset_length - train_size\n",
    "#     train_set, test_set = torch.utils.data.random_split(dataset, [train_size,test_size])\n",
    "\n",
    "#     train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     # Create the Trainer object\n",
    "#     trainer = Trainer(\n",
    "#         model, loss_criterion,\n",
    "#         train_data_loader=train_loader,\n",
    "#         test_data_loader=test_loader,\n",
    "#         optimizer=optimizer,\n",
    "#         epochs=30,\n",
    "#         with_cuda=True,\n",
    "#         use_scheduler=True,\n",
    "#         scheduler=scheduler,\n",
    "#         save_dir='/home/ec2-user/SageMaker/checkpoints/uncropped/',\n",
    "#         tensorboard_dir=tensorboard_dir,\n",
    "#         lr=lr)\n",
    "    \n",
    "#     # Train the model\n",
    "#     trainer._train_epoch()\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     accuracy = trainer._valid_epoch(model, 0, test_loader)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Perform grid search\n",
    "# best_accuracy = 0.0\n",
    "# best_hyperparameters = None\n",
    "\n",
    "# for hyperparameters in hyperparameter_combinations:\n",
    "#     model_class, lr, weight_decay, dropout, loss_weight, batch_size = hyperparameters\n",
    "#     print(f\"Training model {model_class.__name__} with lr={lr}, weight_decay={weight_decay}, dropout={dropout}, loss_weight={loss_weight}, batch_size={batch_size}...\")\n",
    "#     accuracy = train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size)\n",
    "#     print(f\"Accuracy: {accuracy}%\")\n",
    "    \n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_hyperparameters = hyperparameters\n",
    "        \n",
    "# print(best_accuracy)\n",
    "# print(best_hyperparameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
