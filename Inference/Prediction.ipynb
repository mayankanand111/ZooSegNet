{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "662f34f7-5fbf-42ff-9b91-dcc835cca1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd5c48f-04cf-4f36-a74d-c0d3080067a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bc345f-ec67-4d45-9e36-4a2bb427b6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m15.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.5/126.5 kB\u001B[0m \u001B[31m29.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.56.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.2/5.2 MB\u001B[0m \u001B[31m46.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.8/181.8 kB\u001B[0m \u001B[31m35.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m94.2/94.2 kB\u001B[0m \u001B[31m24.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (67.7.2)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/6.6 MB\u001B[0m \u001B[31m43.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (0.40.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.3/181.3 kB\u001B[0m \u001B[31m37.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.14)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m151.7/151.7 kB\u001B[0m \u001B[31m36.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: tensorboard-data-server, pyasn1-modules, oauthlib, markdown, grpcio, cachetools, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-1.4.0 cachetools-5.3.1 google-auth-2.22.0 google-auth-oauthlib-1.0.0 grpcio-1.56.2 markdown-3.4.4 oauthlib-3.2.2 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 tensorboard-2.13.0 tensorboard-data-server-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4662a00-d400-4a3d-b777-aaef7902ff86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc8d923-b324-445e-b70f-6bc1fba1d0de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--tensorboard_dir'], dest='tensorboard_dir', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, required=True, help='Path to save tensorboard dir', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Add any other command-line arguments that your script needs\n",
    "parser = argparse.ArgumentParser(description=\"prediction script\")\n",
    "parser.add_argument('--chkpoint', type=str, help='Path to model saved checkpoint', required=True)\n",
    "parser.add_argument('--image_folder', type=str, help='Path to input image folder', required=True)\n",
    "parser.add_argument('--save_dir', type=str, help='Path to output Image folders', required=True)\n",
    "parser.add_argument('--tensorboard_dir', type=str, help='Path to save tensorboard dir', required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd4f951-ad0e-49b1-91fb-5f92c30f4935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --chkpoint CHKPOINT --image_folder\n",
      "                             IMAGE_FOLDER --save_dir SAVE_DIR\n",
      "                             --tensorboard_dir TENSORBOARD_DIR\n",
      "ipykernel_launcher.py: error: the following arguments are required: --chkpoint, --image_folder, --save_dir, --tensorboard_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84971c10-5882-4eed-845b-1424cb187ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "chkpoint=args.chkpoint\n",
    "#\"/home/ec2-user/SageMaker/checkpoints/uncropped/Model_v2_binary_best.pth.tar\"\n",
    "dropout=0.2\n",
    "image_folder=args.image_folder\n",
    "#'/home/ec2-user/SageMaker/prediction_July4/Dataset/'\n",
    "tensorboard_dir = args.tensorboard_dir\n",
    "# '/home/ec2-user/SageMaker/prediction_July4/runs/'\n",
    "save_dir=save_dir\n",
    "#'/home/ec2-user/SageMaker/prediction_July4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ddd8cd-933d-435f-97fc-5f40c87bafd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ZooplanktonDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_names = [name for name in os.listdir(image_folder) if name != \".ipynb_checkpoints\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.image_names[index]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dee8d0f-0a6e-4893-a3e7-9dc188236bad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m data_transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m      2\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m)),\n\u001B[1;32m      3\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[1;32m      4\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize(mean\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.485\u001B[39m, \u001B[38;5;241m0.456\u001B[39m, \u001B[38;5;241m0.406\u001B[39m], std\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.229\u001B[39m, \u001B[38;5;241m0.224\u001B[39m, \u001B[38;5;241m0.225\u001B[39m])\n\u001B[1;32m      5\u001B[0m ])\n\u001B[0;32m----> 7\u001B[0m dataset \u001B[38;5;241m=\u001B[39m ZooplanktonDataset(image_folder\u001B[38;5;241m=\u001B[39m\u001B[43mimage_folder\u001B[49m,\n\u001B[1;32m      8\u001B[0m                             transform\u001B[38;5;241m=\u001B[39mdata_transform)\n\u001B[1;32m     10\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset\u001B[38;5;241m=\u001B[39mdataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'image_folder' is not defined"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ZooplanktonDataset(image_folder=image_folder,\n",
    "                            transform=data_transform)\n",
    "\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "811a3b3b-1517-457a-ae3d-47f5980ec960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model_v2(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(Model_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Adjusting the fully connected layers to fit the spatial dimensions\n",
    "        # of the input image (256x256)\n",
    "        self.fc1 = nn.Linear(64 * 30 * 30, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)  # Output layer for binary classification (1 neuron for probability)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for probability output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(F.relu(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 64 * 30 * 30)  # Adjusted view shape\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.sigmoid(self.fc3(x))  # Use sigmoid activation for probability output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc1c70-65a9-4701-a4cb-d0ff321f17a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, test_data_loader, save_dir, with_cuda,tensorboard_dir, print_freq=1):\n",
    "        self.device = torch.device(\"cuda:0\" if with_cuda else \"cpu\")\n",
    "        self.model = model\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.save_dir = save_dir\n",
    "        self.print_freq = print_freq\n",
    "        self.step = 0\n",
    "        self.accuracy = 0\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.start_time = datetime.now().strftime('%b-%d_%H-%M')\n",
    "        if not os.path.exists(self.tensorboard_dir):\n",
    "            os.makedirs(self.tensorboard_dir)\n",
    "        self.writer = SummaryWriter(log_dir=self.tensorboard_dir)\n",
    "\n",
    "    def predict(self, threshold=0.5):\n",
    "        checkpoint = torch.load(chkpoint)\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        self.model.eval()\n",
    "        images = []\n",
    "        predicted_labels = []\n",
    "        names = []\n",
    "\n",
    "        for batch_images, image_names in self.test_data_loader:\n",
    "            with torch.no_grad():\n",
    "                if self.device:\n",
    "                    batch_images = batch_images.to(self.device)\n",
    "\n",
    "                # Forward pass to obtain predicted probabilities for each item in the batch\n",
    "                batch_outputs = self.model(batch_images)\n",
    "                batch_predicted_labels = (batch_outputs >= threshold).int()  # Apply threshold for binary classification\n",
    "\n",
    "            # Iterate over each item in the batch\n",
    "            for i in range(batch_images.size(0)):\n",
    "                image = vutils.make_grid(batch_images[i], nrow=1, padding=10, normalize=True)\n",
    "                predicted_label = batch_predicted_labels[i].item()  # Convert to scalar value\n",
    "                name = image_names[i]\n",
    "\n",
    "                images.append(image)\n",
    "                predicted_labels.append(predicted_label)\n",
    "                names.append(name)\n",
    "        \n",
    "        \n",
    "        zooplankton_dir = os.path.join(self.save_dir, 'Detected')\n",
    "        marine_snow_dir = os.path.join(self.save_dir, 'Not-Detected')\n",
    "\n",
    "        # Delete directories if they exist\n",
    "        if os.path.exists(zooplankton_dir):\n",
    "            shutil.rmtree(zooplankton_dir)\n",
    "        if os.path.exists(marine_snow_dir):\n",
    "            shutil.rmtree(marine_snow_dir)\n",
    "\n",
    "        # Create directories\n",
    "        os.makedirs(zooplankton_dir)\n",
    "        os.makedirs(marine_snow_dir)\n",
    "\n",
    "        # Save the classified images into respective directories based on their predicted labels\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            predicted_label = predicted_labels[i]\n",
    "            image_name = names[i]\n",
    "            # Save the image to the appropriate directory based on the predicted label\n",
    "            if predicted_label == 1:\n",
    "                save_path = os.path.join(zooplankton_dir, image_name)\n",
    "            else:\n",
    "                save_path = os.path.join(marine_snow_dir, image_name)\n",
    "\n",
    "            vutils.save_image(image, save_path, normalize=True)\n",
    "\n",
    "        # Log the images on TensorBoard\n",
    "        for i, image in enumerate(images):\n",
    "            image_name = names[i]\n",
    "            label = predicted_labels[i]\n",
    "            self.writer.add_image(f\"Image-{i}-Predicted-{label}\", image.cpu(), global_step=i)\n",
    "\n",
    "        # Close the SummaryWriter object\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab750f96-fc18-47d0-81ab-c870b1764a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    #model\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model_v2(dropout)\n",
    "    model.to(device)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    #creating object for tensorboard_dir\n",
    "    tensorboard_dir = tensorboard_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57fef3-6716-46b7-9c65-e6a199e68627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " trainer = Trainer(\n",
    "        model,\n",
    "        test_data_loader=test_loader,\n",
    "        save_dir=save_dir,\n",
    "        with_cuda=True,\n",
    "        tensorboard_dir=tensorboard_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cffe42-24fb-421a-bf6d-f3b1b29576f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.predict(threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026da96-5bc4-4933-9c88-d97339542daa",
   "metadata": {},
   "source": [
    "# Make csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58dc09b-672c-4a74-a028-8e26e046090c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "folder1_path = '/home/ec2-user/SageMaker/prediction_July4/Detected'\n",
    "folder2_path = '/home/ec2-user/SageMaker/prediction_July4/Not-Detected'\n",
    "csv_file_path = '/home/ec2-user/SageMaker/prediction_July4/UnModified_prediction.csv'\n",
    "\n",
    "folder1_images = os.listdir(folder1_path)\n",
    "folder2_images = os.listdir(folder2_path)\n",
    "\n",
    "data = []\n",
    "for image_name in folder1_images:\n",
    "    data.append([image_name, 'Detected'])\n",
    "\n",
    "for image_name in folder2_images:\n",
    "    data.append([image_name, 'NotDetected'])\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Image Name', 'Label'])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"CSV file created: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fbeb2-eb28-49c1-b1a0-01e24dafa879",
   "metadata": {},
   "source": [
    "# Comparing and creating confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25e515-421d-421f-90bc-72a0be60625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/ec2-user/SageMaker/prediction_July4/2023-07-06_Natasha.csv')\n",
    "df2 = pd.read_csv('/home/ec2-user/SageMaker/prediction_July4/UnModified_prediction.csv')\n",
    "df1 = df1.iloc[:, [1, 9]]\n",
    "df1 = df1.rename(columns={'2: Video or Image Identifier': 'Image Name','10-11+: Repeated Species':'Label'})\n",
    "df1 = df1.drop_duplicates(subset='Image Name').reset_index(drop=True)\n",
    "df1['Label'] = 'Detected'\n",
    "merged_df = df1.merge(df2, on='Image Name', how='outer')\n",
    "merged_df['Label_x'].fillna('Not-Detected', inplace=True)\n",
    "confusion_matrix = merged_df.groupby(['Label_x', 'Label_y']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.diag(confusion_matrix).sum()\n",
    "total_predictions = confusion_matrix.values.sum()\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "#visulaization\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010e598-a5d0-467e-bcbe-481976f3eef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/ec2-user/SageMaker/prediction_July4/2023-07-06_Natasha.csv')\n",
    "df2 = pd.read_csv('/home/ec2-user/SageMaker/prediction_July4/UnModified_prediction.csv')\n",
    "df1 = df1.iloc[:, [1, 9]]\n",
    "print(df1.shape)\n",
    "df1 = df1.rename(columns={'2: Video or Image Identifier': 'Image Name','10-11+: Repeated Species':'Label'})\n",
    "values_to_drop = ['hydrozoa-maybe', 'copepod-maybe', 'copepod-maybe-edge', 'echinoderm-larva-maybe', 'unknown']\n",
    "df1 = df1[~df1.iloc[:, 1].isin(values_to_drop)]\n",
    "df1 = df1.drop_duplicates(subset='Image Name').reset_index(drop=True)\n",
    "print(df1.shape)\n",
    "df1['Label'] = 'Detected'\n",
    "merged_df = df1.merge(df2, on='Image Name', how='outer')\n",
    "merged_df['Label_x'].fillna('Not-Detected', inplace=True)\n",
    "confusion_matrix = merged_df.groupby(['Label_x', 'Label_y']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.diag(confusion_matrix).sum()\n",
    "total_predictions = confusion_matrix.values.sum()\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "#visulaization\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d9c31-008d-4d66-b46a-81dd813e93cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "# Convert labels to binary values\n",
    "merged_df['Label_y'] = np.where(merged_df['Label_y'] == 'Detected', 1, 0)\n",
    "merged_df['Label_x'] = np.where(merged_df['Label_x'] == 'Detected', 1, 0)\n",
    "\n",
    "# Calculate TPR and FPR\n",
    "tn, fp, fn, tp = confusion_matrix.values.ravel()\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(merged_df['Label_y'], merged_df['Label_x'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3002029-5f93-4a77-983a-034ffa051b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26f356-6eb2-4d6b-bd1e-7f7268dc9de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
