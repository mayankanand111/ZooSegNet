{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632bc5c7-3383-479b-bc2b-053037e66c18",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a66df1e2-3718-4cfe-83de-61750940bbe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.56.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (3.4.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (67.7.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard) (0.40.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a543005-cabc-41ad-8649-287c81ca6ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "# from skimage import io,transform,img_as_ubyte\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f05bc-d0b6-4be3-9952-98b92c3fa7ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77b8ced9-17b3-4796-a75b-417ec9e73c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ZooplanktonDataset(Dataset):\n",
    "    def __init__(self, csv, cropped_image_folder, transform=None):\n",
    "        self.annotations = pd.read_csv(csv)\n",
    "        self.cropped_image_folder = cropped_image_folder\n",
    "        self.transform = transform\n",
    "        self.labels = np.array(self.annotations.iloc[:, 1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.annotations.iloc[index, 0]\n",
    "        cropped_img_path = os.path.join(self.cropped_image_folder, img_name)\n",
    "        \n",
    "        image = Image.open(cropped_img_path).convert('RGB')\n",
    "        \n",
    "        y_label = self.annotations.iloc[index, 1]\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, y_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a3eaf-f9cb-4dc7-850f-dc0254449034",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c729efa8-ef8e-4da8-8273-4c7746d26259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchvision.transforms import RandomRotation, RandomAffine\n",
    "\n",
    "# class ZooplanktonDataset(Dataset):\n",
    "#     def __init__(self, csv, cropped_image_folder, transform=None):\n",
    "#         self.annotations = pd.read_csv(csv)\n",
    "#         self.cropped_image_folder = cropped_image_folder\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # Perform oversampling\n",
    "#         label_counts = self.annotations['Label'].value_counts()\n",
    "#         max_count = label_counts.max()\n",
    "#         self.annotations = self.oversample_minority_class(self.annotations, max_count)\n",
    "    \n",
    "#     def oversample_minority_class(self, annotations, target_count):\n",
    "#         # Separate samples from each class\n",
    "#         class_0 = annotations[annotations['Label'] == 0]\n",
    "#         class_1 = annotations[annotations['Label'] == 1]\n",
    "\n",
    "#         # Determine the number of samples to generate for the minority class\n",
    "#         minority_count = target_count - len(class_1)\n",
    "\n",
    "#         # Upsample minority class with augmentation\n",
    "#         class_1_augmented = self.augment_data(class_1, minority_count)\n",
    "\n",
    "#         # Concatenate the augmented minority class with the majority class\n",
    "#         oversampled_annotations = pd.concat([class_0, class_1_augmented])\n",
    "\n",
    "#         return oversampled_annotations\n",
    "    \n",
    "#     def augment_data(self, data, target_count):\n",
    "#         augmented_data = []\n",
    "#         rotation_angle = 30  # Specify the rotation angle\n",
    "        \n",
    "#         # Apply augmentation to reach the target count\n",
    "#         while len(augmented_data) < target_count:\n",
    "#             # Randomly select an image from the data\n",
    "#             image_row = data.sample(n=1)\n",
    "#             image_name = image_row['ImageName'].values[0]\n",
    "#             image_path = os.path.join(self.cropped_image_folder, image_name)\n",
    "            \n",
    "#             # Load the image\n",
    "#             image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "#             # Generate a new image name based on the original image name\n",
    "#             augmented_image_name = image_name\n",
    "            \n",
    "#             # Apply additional transformations (rotation and shear)\n",
    "#             rotation_transform = RandomRotation(rotation_angle)\n",
    "#             affine_transform = RandomAffine(degrees=0, shear=20)\n",
    "            \n",
    "#             augmented_image = affine_transform(rotation_transform(image))\n",
    "            \n",
    "#             augmented_data.append((augmented_image, augmented_image_name, image_row['Label'].values[0]))\n",
    "        \n",
    "#         augmented_data_df = pd.DataFrame(augmented_data, columns=['AugmentedImage', 'ImageName', 'Label'])\n",
    "#         return augmented_data_df\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.annotations)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         img_name = str(self.annotations.iloc[index]['ImageName'])\n",
    "#         cropped_img_path = os.path.join(self.cropped_image_folder, img_name)\n",
    "#         image = Image.open(cropped_img_path).convert('RGB')\n",
    "#         label = self.annotations.iloc[index, 1]\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c6be0-975b-4e30-b988-abbd01552708",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfb98a51-ebfe-4513-bfbd-168b0da94012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# class ZooplanktonDataset(Dataset):\n",
    "#     def __init__(self, csv, cropped_image_folder, transform=None):\n",
    "#         self.annotations = pd.read_csv(csv)\n",
    "#         self.cropped_image_folder = cropped_image_folder\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # Perform undersampling\n",
    "#         label_counts = self.annotations['Label'].value_counts()\n",
    "#         min_count = label_counts.min()\n",
    "#         self.annotations = self.undersample_majority_class(self.annotations, min_count)\n",
    "    \n",
    "#     def undersample_majority_class(self, annotations, target_count):\n",
    "#         # Separate samples from each class\n",
    "#         class_0 = annotations[annotations['Label'] == 0]\n",
    "#         class_1 = annotations[annotations['Label'] == 1]\n",
    "        \n",
    "#         # Downsample majority class\n",
    "#         class_0_undersampled = resample(class_0, replace=False, n_samples=target_count, random_state=42)\n",
    "        \n",
    "#         # Concatenate the undersampled majority class with the minority class\n",
    "#         undersampled_annotations = pd.concat([class_0_undersampled, class_1])\n",
    "        \n",
    "#         return undersampled_annotations\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.annotations)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         img_name = self.annotations.iloc[index, 0]\n",
    "#         cropped_img_path = os.path.join(self.cropped_image_folder, img_name)\n",
    "        \n",
    "#         image = Image.open(cropped_img_path).convert('RGB')\n",
    "        \n",
    "#         label = self.annotations.iloc[index, 1]\n",
    "            \n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76534ffa-dbb5-49ae-98f0-579338d62172",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train and Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c23216f-6606-45c2-a314-6925f62aa8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# data_transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.RandomRotation(10),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "dataset = ZooplanktonDataset(csv='/home/ec2-user/SageMaker/All_csv_combined/annotations.csv',cropped_image_folder='dataset/uncropped',\n",
    "                            transform=data_transform)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Splitting the indices into train and test sets with a stratified split\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(dataset)), test_size=0.2, stratify=dataset.labels\n",
    ")\n",
    "\n",
    "# Create subset datasets using the train and test indices\n",
    "train_set = Subset(dataset, train_indices)\n",
    "test_set = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663f213-8f71-46c3-9463-446d4a663454",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23617768-f290-4624-bac7-a589797d8f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Basemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basemodel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32*53*53, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 53 * 53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d4fcede-d733-4dd8-bac9-feb68c3ca1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_v1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(64 * 26 * 26, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64 * 26 * 26)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f54fb7f0-625e-4779-8565-0d9bd6e007f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Model_v2(nn.Module):\n",
    "#     def __init__(self, dropout):\n",
    "#         super(Model_v2, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "#         self.pool2 = nn.MaxPool2d(3, 2)\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.fc1 = nn.Linear(64 * 26 * 26, 256)\n",
    "#         self.fc2 = nn.Linear(256, 84)\n",
    "#         self.fc3 = nn.Linear(84, 2)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.dropout(F.relu(self.conv1(x)))\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.dropout(F.relu(self.conv2(x)))\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.dropout(F.relu(self.conv3(x)))\n",
    "#         x = self.pool3(x)\n",
    "#         x = x.view(-1, 64 * 26 * 26)\n",
    "#         x = self.dropout(F.relu(self.fc1(x)))\n",
    "#         x = self.dropout(F.relu(self.fc2(x)))\n",
    "#         x = self.softmax(self.fc3(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aff2c3e4-9ce5-41eb-b707-ed120b8db684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accepts 256x256\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model_v2(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(Model_v2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Adjusting the fully connected layers to fit the spatial dimensions\n",
    "        # of the input image (256x256)\n",
    "        self.fc1 = nn.Linear(64 * 30 * 30, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(F.relu(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(-1, 64 * 30 * 30)  # Adjusted view shape\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c42393d6-f152-491e-b67a-9a2cf603e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class Model_v2(nn.Module):\n",
    "#     def __init__(self, dropout):\n",
    "#         super(Model_v2, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.fc1 = nn.Linear(128 * 37 * 37, 512)  # Calculate the input size based on 300x300 input images\n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout)  # Use the provided dropout value\n",
    "#         self.fc2 = nn.Linear(512, 2)  # Output layer with 2 neurons for binary classification\n",
    "\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(self.relu1(self.conv1(x)))\n",
    "#         x = self.pool2(self.relu2(self.conv2(x)))\n",
    "#         x = self.pool3(self.relu3(self.conv3(x)))\n",
    "\n",
    "#         x = x.view(-1, 128 * 37 * 37)  # Flatten the tensor\n",
    "#         x = self.dropout(self.relu4(self.fc1(x)))\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73da546e-c548-4daa-8aff-954e8a50058d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Model_v2(nn.Module):\n",
    "#     def __init__(self, dropout):\n",
    "#         super(Model_v2, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "#         self.pool2 = nn.MaxPool2d(3, 2)\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # Adjusting the fully connected layers to fit the spatial dimensions\n",
    "#         # of the input image (256x256)\n",
    "#         self.fc1 = nn.Linear(64 * 30 * 30, 256)\n",
    "#         self.fc2 = nn.Linear(256, 84)\n",
    "#         self.fc3 = nn.Linear(84, 1)  # One output unit for binary classification\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.dropout(F.relu(self.conv1(x)))\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.dropout(F.relu(self.conv2(x)))\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.dropout(F.relu(self.conv3(x)))\n",
    "#         x = self.pool3(x)\n",
    "#         x = x.view(-1, 64 * 30 * 30)  # Adjusted view shape\n",
    "#         x = self.dropout(F.relu(self.fc1(x)))\n",
    "#         x = self.dropout(F.relu(self.fc2(x)))\n",
    "#         x = self.fc3(x) \n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "329d68df-5616-4442-9d45-8db12f4ea230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model_v3(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(Model_v3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(F.relu(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout(F.relu(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        x = x.view(-1, 128 * 12 * 12)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb8ad3-2aec-4f7d-8ddd-2a229c5af1a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loss curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "064031cd-6a36-479e-9e02-ffa0eac97b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LossCurve:\n",
    "    def __init__(self):\n",
    "        self\n",
    "\n",
    "    def PlotCurve(loss_values,epochs):\n",
    "        plt.plot(range(epochs), loss_values, 'blue')\n",
    "        plt.title('Loss decay')\n",
    "        plt.xlabel('number of epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bd25c-d5fe-45e2-a39b-885db688590d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a592bbe-76ae-4b74-9f7e-f18626f3a04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, loss_criterion,\n",
    "                 train_data_loader, test_data_loader,\n",
    "                 optimizer, epochs, with_cuda,\n",
    "                 use_scheduler, scheduler, save_dir,tensorboard_dir, lr=0.001,print_freq=1):\n",
    "        self.device = torch.device(\"cuda:0\" if with_cuda else \"cpu\")\n",
    "        self.model = model\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.with_cuda = with_cuda\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.lr = lr\n",
    "        self.save_dir = save_dir\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.use_scheduler = use_scheduler\n",
    "        self.scheduler = scheduler\n",
    "        self.step = 0\n",
    "        self.print_freq = print_freq\n",
    "        self.start_time = datetime.now().strftime('%b-%d_%H-%M')\n",
    "        self.save_freq = 1\n",
    "        self.accuracy = 0\n",
    "        if not os.path.exists(self.tensorboard_dir):\n",
    "            os.makedirs(self.tensorboard_dir)\n",
    "        self.writer = SummaryWriter(log_dir=self.tensorboard_dir)\n",
    "        \n",
    "    def _train_epoch(self):\n",
    "        loss_values = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0\n",
    "            last_step = self.step - 1\n",
    "            last_time = time.time()\n",
    "            batch_counter = 0\n",
    "            accuracy=0\n",
    "            loop = tqdm(enumerate(self.train_data_loader), total=len(self.train_data_loader), leave=True)\n",
    "            for batch_idx, (images, labels) in loop:\n",
    "                if self.with_cuda:\n",
    "                    images, labels = images.to(self.device), labels.to (self.device)\n",
    "                self.model.train()\n",
    "                batch_counter += 1\n",
    "                # forward pass\n",
    "                outputs = self.model(images)\n",
    "                # finding loss\n",
    "                loss = self.loss_criterion(outputs, labels)\n",
    "                # backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.use_scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                # evaluation of model on test data\n",
    "                if batch_counter % 1000 == 0 or batch_counter == len(self.train_data_loader):\n",
    "                    accuracy = self._valid_epoch(self.model, epoch, self.test_data_loader)\n",
    "                \n",
    "                is_best = False\n",
    "                if accuracy > self.accuracy:\n",
    "                    is_best = True\n",
    "                self.accuracy = accuracy\n",
    "                \n",
    "                if self.step % self.print_freq == self.print_freq - 1:\n",
    "                    used_time = time.time() - last_time\n",
    "                    step_num = self.step - last_step\n",
    "                    speed = self.train_data_loader.batch_size * \\\n",
    "                        step_num / used_time\n",
    "                    batch_loss = loss.item()\n",
    "                    loop.set_description(f\"Epoch[{epoch + 1}/{self.epochs}]\")\n",
    "                    loop.set_postfix(\n",
    "                    lr=self.scheduler.get_last_lr(),\n",
    "                    batch_loss=batch_loss,\n",
    "                    global_loss = running_loss,\n",
    "                    speed=speed,\n",
    "                    accuracy = self.accuracy,\n",
    "                    device = self.device\n",
    "                    )\n",
    "                    global_loss = 0.0\n",
    "                    last_step = self.step\n",
    "                    last_time = time.time()\n",
    "                self.step += 1\n",
    "\n",
    "            if epoch % self.save_freq == 0:\n",
    "                self._save_checkpoint(epoch, is_best)\n",
    "            \n",
    "            self.writer.add_scalar('Loss', running_loss / len(self.train_data_loader), global_step=self.step)\n",
    "            loss_values.append(running_loss / len(self.train_data_loader))\n",
    "        \n",
    "        print(\"Train Accuracy:\", self._valid_epoch(self.model, self.epochs, self.train_data_loader))\n",
    "        print(\"Validation Accuracy:\", self._valid_epoch(self.model, self.epochs, self.test_data_loader))\n",
    "        # Plotting Loss Curve\n",
    "        LossCurve.PlotCurve(loss_values, self.epochs)\n",
    "        \n",
    "        #logging images to Tensorboard\n",
    "        self.log_images_after_training()\n",
    "        \n",
    "        #closing SummaryWriter Object\n",
    "        self.writer.close()\n",
    "        \n",
    "    def _valid_epoch(self, model, epoch, test_loader):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct_samples = 0\n",
    "            total_samples = 0\n",
    "            for images, labels in test_loader:\n",
    "                if self.with_cuda:\n",
    "                    images, labels = images.to(self.device), labels.to (self.device)\n",
    "                pred_ratio = model(images)\n",
    "                _, pred_labels = torch.max(pred_ratio, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_samples += (pred_labels == labels).sum().item()\n",
    "            accuracy = (correct_samples / total_samples) * 100\n",
    "            self.writer.add_scalar('Accuracy', accuracy, global_step=epoch)\n",
    "            return accuracy\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, is_best):\n",
    "        arch = type(self.model).__name__\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'arch': arch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'accuracy': self.accuracy,\n",
    "            'step': self.step + 1,\n",
    "            'start_time': self.start_time}\n",
    "        filename = os.path.join(\n",
    "            self.save_dir+\n",
    "            'checkpoint_{}_epoch{:02d}_acc_{:.5f}.pth.tar'.format(\n",
    "                arch,\n",
    "                epoch, \n",
    "                self.accuracy))\n",
    "        print(\"Saving checkpoint: {} ...\".format(filename))\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "        torch.save(state, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(\n",
    "                filename, os.path.join(self.save_dir,'{}_best.pth.tar'.format(arch)))\n",
    "        return filename\n",
    "\n",
    "    def log_images_after_training(self):\n",
    "        arch = type(self.model).__name__\n",
    "        checkpoint = torch.load('/home/ec2-user/SageMaker/checkpoints/uncropped/{}_best.pth.tar'.format(arch))\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        self.model.eval()\n",
    "        images = []\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        for batch_images, batch_labels in self.test_data_loader:\n",
    "            with torch.no_grad():\n",
    "                if self.with_cuda:\n",
    "                    batch_images, batch_labels = batch_images.to(self.device), batch_labels.to(self.device)\n",
    "\n",
    "                # Forward pass to obtain predicted labels for each item in the batch\n",
    "                batch_outputs = self.model(batch_images)\n",
    "                batch_predicted_labels = torch.argmax(batch_outputs, dim=1)\n",
    "            # Iterate over each item in the batch\n",
    "            for i in range(batch_images.size(0)):\n",
    "                image = vutils.make_grid(batch_images[i], nrow=1, padding=10, normalize=True) \n",
    "                true_label = batch_labels[i]\n",
    "                predicted_label = batch_predicted_labels[i]\n",
    "\n",
    "                images.append(image)\n",
    "                true_labels.append(true_label)\n",
    "                predicted_labels.append(predicted_label)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            true_label = true_labels[i]\n",
    "            predicted_label = predicted_labels[i]\n",
    "\n",
    "            # Set the image name as the predicted and true labels\n",
    "            image_name = f\"Image-{i}-True-{true_label}-Predicted-{predicted_label}\"\n",
    "\n",
    "            # Log the image on TensorBoard with the image name\n",
    "            self.writer.add_image(image_name, image.cpu(), global_step=i)\n",
    "\n",
    "        #creating confusion matrix\n",
    "        true_labels = torch.tensor(true_labels)\n",
    "        predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.colorbar()\n",
    "\n",
    "        # Set the axis labels and ticks\n",
    "        class_names = ['Zooplankton', 'Marine-Snow']\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        # Add labels to each cell of the confusion matrix\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        # Save the confusion matrix figure to TensorBoard\n",
    "        self.writer.add_figure('Confusion Matrix', fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97b0d215-6bb9-47e4-80cb-771d079328d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import shutil\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.utils as vutils\n",
    "# from datetime import datetime\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import itertools\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# class Trainer():\n",
    "#     def __init__(self, model, loss_criterion, threshold,\n",
    "#                  train_data_loader, test_data_loader,\n",
    "#                  optimizer, epochs, with_cuda,\n",
    "#                  use_scheduler, scheduler, save_dir, tensorboard_dir, lr=0.001, print_freq=1):\n",
    "#         self.device = torch.device(\"cuda:0\" if with_cuda else \"cpu\")\n",
    "#         self.model = model\n",
    "#         self.loss_criterion = loss_criterion\n",
    "#         self.optimizer = optimizer\n",
    "#         self.epochs = epochs\n",
    "#         self.with_cuda = with_cuda\n",
    "#         self.train_data_loader = train_data_loader\n",
    "#         self.test_data_loader = test_data_loader\n",
    "#         self.lr = lr\n",
    "#         self.save_dir = save_dir\n",
    "#         self.tensorboard_dir = tensorboard_dir\n",
    "#         self.use_scheduler = use_scheduler\n",
    "#         self.scheduler = scheduler\n",
    "#         self.step = 0\n",
    "#         self.print_freq = print_freq\n",
    "#         self.start_time = datetime.now().strftime('%b-%d_%H-%M')\n",
    "#         self.save_freq = 1\n",
    "#         self.accuracy = 0\n",
    "#         if not os.path.exists(self.tensorboard_dir):\n",
    "#             os.makedirs(self.tensorboard_dir)\n",
    "#         self.writer = SummaryWriter(log_dir=self.tensorboard_dir)\n",
    "#         self.threshold = threshold  # Threshold for classifying as true or false (0 to 1)\n",
    "\n",
    "#     def _train_epoch(self):\n",
    "#         loss_values = []\n",
    "\n",
    "#         for epoch in range(self.epochs):\n",
    "#             running_loss = 0\n",
    "#             last_step = self.step - 1\n",
    "#             last_time = time.time()\n",
    "#             batch_counter = 0\n",
    "#             accuracy = 0\n",
    "#             loop = tqdm(enumerate(self.train_data_loader), total=len(self.train_data_loader), leave=True)\n",
    "#             for batch_idx, (images, labels) in loop:\n",
    "#                 if self.with_cuda:\n",
    "#                     images, labels = images.to(self.device), labels.to(self.device)\n",
    "#                 self.model.train()\n",
    "#                 batch_counter += 1\n",
    "#                 labels = labels.view(-1, 1).float()\n",
    "#                 # forward pass\n",
    "#                 outputs = self.model(images)\n",
    "#                 # finding loss\n",
    "#                 loss = self.loss_criterion(outputs, labels)\n",
    "#                 # backward pass\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "\n",
    "#                 if self.use_scheduler:\n",
    "#                     self.scheduler.step()\n",
    "\n",
    "#                 running_loss += loss.item()\n",
    "#                 # evaluation of model on test data\n",
    "#                 if batch_counter % 1000 == 0 or batch_counter == len(self.train_data_loader):\n",
    "#                     accuracy = self._valid_epoch(self.model, epoch, self.test_data_loader)\n",
    "\n",
    "#                 is_best = False\n",
    "#                 if accuracy > self.accuracy:\n",
    "#                     is_best = True\n",
    "#                 self.accuracy = accuracy\n",
    "\n",
    "#                 if self.step % self.print_freq == self.print_freq - 1:\n",
    "#                     used_time = time.time() - last_time\n",
    "#                     step_num = self.step - last_step\n",
    "#                     speed = self.train_data_loader.batch_size * step_num / used_time\n",
    "#                     batch_loss = loss.item()\n",
    "#                     loop.set_description(f\"Epoch[{epoch + 1}/{self.epochs}]\")\n",
    "#                     loop.set_postfix(\n",
    "#                         lr=self.scheduler.get_last_lr(),\n",
    "#                         batch_loss=batch_loss,\n",
    "#                         global_loss=running_loss,\n",
    "#                         speed=speed,\n",
    "#                         accuracy=self.accuracy,\n",
    "#                         device=self.device\n",
    "#                     )\n",
    "#                     global_loss = 0.0\n",
    "#                     last_step = self.step\n",
    "#                     last_time = time.time()\n",
    "#                 self.step += 1\n",
    "\n",
    "#             if epoch % self.save_freq == 0:\n",
    "#                 self._save_checkpoint(epoch, is_best)\n",
    "\n",
    "#             self.writer.add_scalar('Loss', running_loss / len(self.train_data_loader), global_step=self.step)\n",
    "#             loss_values.append(running_loss / len(self.train_data_loader))\n",
    "\n",
    "#         print(\"Train Accuracy:\", self._valid_epoch(self.model, self.epochs, self.train_data_loader))\n",
    "#         print(\"Validation Accuracy:\", self._valid_epoch(self.model, self.epochs, self.test_data_loader))\n",
    "#         # Plotting Loss Curve\n",
    "#         LossCurve.PlotCurve(loss_values, self.epochs)\n",
    "\n",
    "#         # logging images to Tensorboard\n",
    "#         self.log_images_after_training()\n",
    "\n",
    "#         # closing SummaryWriter Object\n",
    "#         self.writer.close()\n",
    "\n",
    "#     def _valid_epoch(self, model, epoch, test_loader):\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             correct_samples = 0\n",
    "#             total_samples = 0\n",
    "#             for images, labels in test_loader:\n",
    "#                 if self.with_cuda:\n",
    "#                     images, labels = images.to(self.device), labels.to(self.device)\n",
    "#                 pred_ratio = model(images)\n",
    "#                 pred_labels = (pred_ratio > self.threshold).float()  # Applying the threshold\n",
    "#                 total_samples += labels.size(0)\n",
    "#                 correct_samples += (pred_labels == labels).sum().item()\n",
    "#             accuracy = (correct_samples / total_samples) * 100\n",
    "#             self.writer.add_scalar('Accuracy', accuracy, global_step=epoch)\n",
    "#             return accuracy\n",
    "\n",
    "#     def _save_checkpoint(self, epoch, is_best):\n",
    "#         arch = type(self.model).__name__\n",
    "#         state = {\n",
    "#             'epoch': epoch,\n",
    "#             'arch': arch,\n",
    "#             'state_dict': self.model.state_dict(),\n",
    "#             'optimizer': self.optimizer.state_dict(),\n",
    "#             'accuracy': self.accuracy,\n",
    "#             'step': self.step + 1,\n",
    "#             'start_time': self.start_time}\n",
    "#         filename = os.path.join(\n",
    "#             self.save_dir +\n",
    "#             'checkpoint_{}_epoch{:02d}_acc_{:.5f}.pth.tar'.format(\n",
    "#                 arch,\n",
    "#                 epoch,\n",
    "#                 self.accuracy))\n",
    "#         print(\"Saving checkpoint: {} ...\".format(filename))\n",
    "#         if not os.path.exists(self.save_dir):\n",
    "#             os.makedirs(self.save_dir)\n",
    "#         torch.save(state, filename)\n",
    "#         if is_best:\n",
    "#             shutil.copyfile(\n",
    "#                 filename, os.path.join(self.save_dir, '{}_best.pth.tar'.format(arch)))\n",
    "#         return filename\n",
    "\n",
    "#     def log_images_after_training(self):\n",
    "#         arch = type(self.model).__name__\n",
    "#         checkpoint = torch.load('/home/ec2-user/SageMaker/checkpoints/uncropped/{}_best.pth.tar'.format(arch))\n",
    "#         self.model.load_state_dict(checkpoint['state_dict'])\n",
    "#         self.model.eval()\n",
    "#         images = []\n",
    "#         true_labels = []\n",
    "#         predicted_labels = []\n",
    "\n",
    "#         for batch_images, batch_labels in self.test_data_loader:\n",
    "#             with torch.no_grad():\n",
    "#                 if self.with_cuda:\n",
    "#                     batch_images, batch_labels = batch_images.to(self.device), batch_labels.to(self.device)\n",
    "\n",
    "#                 # Forward pass to obtain predicted labels for each item in the batch\n",
    "#                 batch_outputs = self.model(batch_images)\n",
    "#                 batch_predicted_labels = (batch_outputs > self.threshold).float()  # Applying the threshold\n",
    "\n",
    "#             # Iterate over each item in the batch\n",
    "#             for i in range(batch_images.size(0)):\n",
    "#                 image = vutils.make_grid(batch_images[i], nrow=1, padding=10, normalize=True)\n",
    "#                 true_label = batch_labels[i]\n",
    "#                 predicted_label = batch_predicted_labels[i]\n",
    "\n",
    "#                 images.append(image)\n",
    "#                 true_labels.append(true_label)\n",
    "#                 predicted_labels.append(predicted_label)\n",
    "\n",
    "#         for i in range(len(images)):\n",
    "#             image = images[i]\n",
    "#             true_label = true_labels[i]\n",
    "#             predicted_label = predicted_labels[i]\n",
    "\n",
    "#             # Set the image name as the predicted and true labels\n",
    "#             image_name = f\"Image-{i}-True-{true_label}-Predicted-{predicted_label}\"\n",
    "\n",
    "#             # Log the image on TensorBoard with the image name\n",
    "#             self.writer.add_image(image_name, image.cpu(), global_step=i)\n",
    "\n",
    "#         # creating confusion matrix\n",
    "#         true_labels = torch.tensor(true_labels)\n",
    "#         predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "#         cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "#         fig = plt.figure(figsize=(8, 8))\n",
    "#         plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#         plt.colorbar()\n",
    "\n",
    "#         # Set the axis labels and ticks\n",
    "#         class_names = ['Zooplankton', 'Marine-Snow']\n",
    "#         tick_marks = np.arange(len(class_names))\n",
    "#         plt.xticks(tick_marks, class_names)\n",
    "#         plt.yticks(tick_marks, class_names)\n",
    "\n",
    "#         # Add labels to each cell of the confusion matrix\n",
    "#         thresh = cm.max() / 2.\n",
    "#         for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#             plt.text(j, i, format(cm[i, j], 'd'),\n",
    "#                      horizontalalignment=\"center\",\n",
    "#                      color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#         # Save the confusion matrix figure to TensorBoard\n",
    "#         self.writer.add_figure('Confusion Matrix', fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2f1e9-e45c-4e62-8a35-7330b88283ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9087ec74-10c8-4b63-9d89-acf6159d26f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 6.3882], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "    #model\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model_v2(0.2)\n",
    "    model.to(device)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "   # optimizer \n",
    "    optimizer = optim.RAdam(\n",
    "        params=parameters,\n",
    "        lr=0.001,\n",
    "        betas=(0.8, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=1e-7)\n",
    "    #scheduler\n",
    "    lr_warm_up_num = 100\n",
    "    cr = 1.0 / math.log(lr_warm_up_num)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda ee: cr * math.log(ee + 1)\n",
    "        if ee < lr_warm_up_num else 1)\n",
    "    #  loss, metrics\n",
    "    data = pd.read_csv('/home/ec2-user/SageMaker/All_csv_combined/annotations.csv')\n",
    "    label_counts = data.iloc[:, 1].value_counts()\n",
    "    loss_ratio = label_counts[0] / label_counts[1]\n",
    "    weight = torch.Tensor([1.0, loss_ratio]).to(device)\n",
    "    print(weight)\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    #creating object for tensorboard_dir\n",
    "    tensorboard_dir = '/home/ec2-user/SageMaker/runs/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf15377-d31a-413c-be7e-42be733190d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trainer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86016839-5292-4fd5-8055-6520441a1397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "   trainer = Trainer(\n",
    "        model, loss_criterion,\n",
    "        train_data_loader=train_loader,\n",
    "        test_data_loader=test_loader,\n",
    "        optimizer=optimizer,\n",
    "        epochs=30,\n",
    "        with_cuda=True,\n",
    "        use_scheduler=True,\n",
    "        scheduler=scheduler,\n",
    "        save_dir='/home/ec2-user/SageMaker/checkpoints/uncropped/',\n",
    "        tensorboard_dir=tensorboard_dir,\n",
    "        lr=0.01)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90322549-280e-4139-9f2a-bdcec70f9bad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92f3d42a-8d58-4462-9aeb-7103e54c1dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# hyperparameter_grid = {\n",
    "#     'model': [Model_v2,Model_v3], # adding more layers # adding more maxpool and make \n",
    "#     'lr': [0.001], # imp\n",
    "#     'weight_decay': [1e-3,1e-4,1e-5,1e-7],\n",
    "#     'dropout': [0.2], #imp\n",
    "#     'loss_weight': [1.0],\n",
    "#     'batch_size': [16], # most imp\n",
    "#     'lr_warm_up_num':[100]\n",
    "# }\n",
    "\n",
    "# # Create a list of all possible combinations of hyperparameters\n",
    "# hyperparameter_combinations = list(itertools.product(*hyperparameter_grid.values()))\n",
    "\n",
    "# # Define a function for training and evaluating the model with a given set of hyperparameters\n",
    "# def train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size, lr_warm_up_num):\n",
    "#     # Create the model\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model_class(dropout)\n",
    "#     model.to(device)\n",
    "#     parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    \n",
    "#     # Create the optimizer\n",
    "#     optimizer = optim.Adam(\n",
    "#         params=parameters,\n",
    "#         lr=lr,\n",
    "#         betas=(0.8, 0.999),\n",
    "#         eps=1e-8,\n",
    "#         weight_decay=weight_decay)\n",
    "#     tensorboard_dir = '/home/ec2-user/SageMaker/runs/'\n",
    "#     # Create the scheduler\n",
    "#     lr_warm_up_num = lr_warm_up_num\n",
    "#     cr = 1.0 / math.log(lr_warm_up_num)\n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(\n",
    "#         optimizer,\n",
    "#         lr_lambda=lambda ee: cr * math.log(ee + 1)\n",
    "#         if ee < lr_warm_up_num else 1)\n",
    "    \n",
    "#     # Create the loss criterion\n",
    "#     data = pd.read_csv('dataset/uncropped/annotations.csv')\n",
    "#     label_counts = data.iloc[:, 1].value_counts()\n",
    "#     loss_ratio = label_counts[0] / label_counts[1]\n",
    "#     weight = torch.Tensor([1,loss_ratio]).to(device)\n",
    "#     loss_criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    \n",
    "# #     # Load the data with the specified batch size\n",
    "# #     dataset_length = len(dataset)\n",
    "# #     train_size = int(0.7 * dataset_length)\n",
    "# #     test_size = dataset_length - train_size\n",
    "# #     train_set, test_set = torch.utils.data.random_split(dataset, [train_size,test_size])\n",
    "\n",
    "# #     train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "# #     test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     # Splitting the indices into train and test sets with a stratified split\n",
    "#     train_indices, test_indices = train_test_split(\n",
    "#         np.arange(len(dataset)), test_size=0.2, stratify=dataset.labels\n",
    "#     )\n",
    "\n",
    "#     # Create subset datasets using the train and test indices\n",
    "#     train_set = Subset(dataset, train_indices)\n",
    "#     test_set = Subset(dataset, test_indices)\n",
    "\n",
    "#     train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     # Create the Trainer object\n",
    "#     trainer = Trainer(\n",
    "#         model, loss_criterion,\n",
    "#         train_data_loader=train_loader,\n",
    "#         test_data_loader=test_loader,\n",
    "#         optimizer=optimizer,\n",
    "#         epochs=30,\n",
    "#         with_cuda=True,\n",
    "#         use_scheduler=True,\n",
    "#         scheduler=scheduler,\n",
    "#         save_dir='/home/ec2-user/SageMaker/checkpoints/uncropped/',\n",
    "#         tensorboard_dir=tensorboard_dir,\n",
    "#         lr=lr)\n",
    "    \n",
    "#     # Train the model\n",
    "#     trainer._train_epoch()\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     accuracy = trainer._valid_epoch(model, 0, test_loader)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Perform grid search\n",
    "# best_accuracy = 0.0\n",
    "# best_hyperparameters = None\n",
    "\n",
    "# for hyperparameters in hyperparameter_combinations:\n",
    "#     model_class, lr, weight_decay, dropout, loss_weight, batch_size, lr_warm_up_num = hyperparameters\n",
    "#     print(f\"Training model {model_class.__name__} with lr={lr}, weight_decay={weight_decay}, dropout={dropout}, loss_weight={loss_weight}, batch_size={batch_size}, lr_warm_up_num={lr_warm_up_num}...\")\n",
    "#     accuracy = train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size, lr_warm_up_num)\n",
    "    \n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_hyperparameters = hyperparameters\n",
    "        \n",
    "# print(best_accuracy)\n",
    "# print(best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae0fc9-9f22-4959-bedb-5f2426b7ccf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f83eeb9-bfc8-4d67-b6f1-9a70905d0d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # Define the hyperparameter grid showing best hyperparameters obtained from Hyperparameter search Grid\n",
    "# hyperparameter_grid = {\n",
    "#     'model': [Model_v2],\n",
    "#     'lr': [0.001],\n",
    "#     'weight_decay': [1e-7],\n",
    "#     'dropout': [0.2],\n",
    "#     'loss_weight': [1.0],\n",
    "#     'batch_size': [16]\n",
    "# }\n",
    "\n",
    "# # Create a list of all possible combinations of hyperparameters\n",
    "# hyperparameter_combinations = list(itertools.product(*hyperparameter_grid.values()))\n",
    "\n",
    "# def train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size, train_loader, test_loader):\n",
    "#     # Create the model\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model_class(dropout)\n",
    "#     model.to(device)\n",
    "#     parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    \n",
    "#     # Create the optimizer\n",
    "#     optimizer = optim.Adam(\n",
    "#         params=parameters,\n",
    "#         lr=lr,\n",
    "#         betas=(0.8, 0.999),\n",
    "#         eps=1e-8,\n",
    "#         weight_decay=weight_decay)\n",
    "#     tensorboard_dir = '/home/ec2-user/SageMaker/runs/'\n",
    "#     # Create the scheduler\n",
    "#     lr_warm_up_num = 100\n",
    "#     cr = 1.0 / math.log(lr_warm_up_num)\n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(\n",
    "#         optimizer,\n",
    "#         lr_lambda=lambda ee: cr * math.log(ee + 1)\n",
    "#         if ee < lr_warm_up_num else 1)\n",
    "    \n",
    "#     # Create the loss criterion\n",
    "#     data = pd.read_csv('dataset/uncropped/annotations.csv')\n",
    "#     label_counts = data.iloc[:, 1].value_counts()\n",
    "#     loss_ratio = label_counts[0] / label_counts[1]\n",
    "#     weight = torch.Tensor([1,loss_ratio]).to(device)\n",
    "#     loss_criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    \n",
    "#     # Create the Trainer object\n",
    "#     trainer = Trainer(\n",
    "#         model, loss_criterion,\n",
    "#         train_data_loader=train_loader,\n",
    "#         test_data_loader=test_loader,\n",
    "#         optimizer=optimizer,\n",
    "#         epochs=30,\n",
    "#         with_cuda=True,\n",
    "#         use_scheduler=True,\n",
    "#         scheduler=scheduler,\n",
    "#         save_dir='/home/ec2-user/SageMaker/checkpoints/uncropped/',\n",
    "#         tensorboard_dir=tensorboard_dir,\n",
    "#         lr=lr)\n",
    "    \n",
    "#     # Train the model\n",
    "#     trainer._train_epoch()\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     accuracy = trainer._valid_epoch(model, 0, test_loader)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Perform 10-fold cross-validation\n",
    "# num_folds = 10\n",
    "# kf = KFold(n_splits=num_folds)\n",
    "\n",
    "# hyperparameter_accuracies = {}\n",
    "\n",
    "# data = pd.read_csv('dataset/uncropped/annotations.csv')\n",
    "# label_counts = data.iloc[:, 1].value_counts()\n",
    "# minority_class_label = label_counts.idxmin()\n",
    "# print(minority_class_label)\n",
    "\n",
    "# for hyperparameters in hyperparameter_combinations:\n",
    "#     model_class, lr, weight_decay, dropout, loss_weight, batch_size = hyperparameters\n",
    "#     print(f\"Training model {model_class.__name__} with lr={lr}, weight_decay={weight_decay}, dropout={dropout}, loss_weight={loss_weight}, batch_size={batch_size}...\")\n",
    "    \n",
    "#     fold_accuracies = []\n",
    "#     for train_index, test_index in kf.split(dataset.annotations):\n",
    "#     # Split the indices into training and testing sets for the current fold\n",
    "#         train_indices = train_index\n",
    "#         test_indices = test_index\n",
    "\n",
    "#         # Select the minority class instances from the test indices\n",
    "#         test_indices_minority = test_indices[data.iloc[test_indices, 1] == minority_class_label]\n",
    "#         test_indices_majority = test_indices[data.iloc[test_indices, 1] != minority_class_label]\n",
    "\n",
    "#         # Calculate the desired number of minority class instances per fold\n",
    "#         num_minority_per_fold = len(test_indices_minority) // num_folds\n",
    "\n",
    "#         # Randomly sample the desired number of minority class instances for the current fold\n",
    "#         test_indices_minority_sampled = np.random.choice(test_indices_minority, size=num_minority_per_fold, replace=False)\n",
    "\n",
    "#         # Combine the sampled minority class instances with the majority class instances\n",
    "#         test_indices_balanced = np.concatenate((test_indices_majority, test_indices_minority_sampled))\n",
    "\n",
    "#         # Shuffle the test indices to ensure randomization\n",
    "#         np.random.shuffle(test_indices_balanced)\n",
    "\n",
    "#         # Create subset datasets using the train and balanced test indices\n",
    "#         train_set = Subset(dataset, train_indices)\n",
    "#         test_set = Subset(dataset, test_indices_balanced)\n",
    "\n",
    "#         # Create data loaders for the current fold\n",
    "#         train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "#         test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         # Train and evaluate the model for the current fold\n",
    "#         accuracy = train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size, train_loader, test_loader)\n",
    "#         fold_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "#     average_accuracy = sum(fold_accuracies) / num_folds\n",
    "#     print(f\"Average accuracy across {num_folds} folds: {average_accuracy}%\")\n",
    "    \n",
    "#     hyperparameter_accuracies[hyperparameters] = average_accuracy\n",
    "\n",
    "# # Print the accuracies for each hyperparameter combination\n",
    "# for hyperparameters, accuracy in hyperparameter_accuracies.items():\n",
    "#     print(f\"Hyperparameters: {hyperparameters}\")\n",
    "#     print(f\"Average accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f57ca-0722-4460-a193-997ba6ccb10d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fea83-1261-44a5-a96a-2e644d6c073e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[1/30]: 100%|| 800/800 [01:23<00:00,  9.54it/s, accuracy=86.7, batch_loss=0.715, device=cuda:0, global_loss=552, lr=[0.001], speed=1.12]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch00_acc_86.74171.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[2/30]: 100%|| 800/800 [01:20<00:00,  9.99it/s, accuracy=85.6, batch_loss=0.547, device=cuda:0, global_loss=522, lr=[0.001], speed=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch01_acc_85.64728.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[3/30]: 100%|| 800/800 [01:17<00:00, 10.28it/s, accuracy=80.1, batch_loss=0.787, device=cuda:0, global_loss=509, lr=[0.001], speed=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch02_acc_80.11257.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[4/30]: 100%|| 800/800 [01:26<00:00,  9.29it/s, accuracy=79.5, batch_loss=0.784, device=cuda:0, global_loss=499, lr=[0.001], speed=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch03_acc_79.45591.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[5/30]: 100%|| 800/800 [01:21<00:00,  9.87it/s, accuracy=81.8, batch_loss=0.417, device=cuda:0, global_loss=477, lr=[0.001], speed=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch04_acc_81.83240.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[6/30]: 100%|| 800/800 [01:21<00:00,  9.77it/s, accuracy=82.2, batch_loss=0.785, device=cuda:0, global_loss=448, lr=[0.001], speed=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch05_acc_82.23890.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[7/30]: 100%|| 800/800 [01:19<00:00, 10.02it/s, accuracy=83.1, batch_loss=0.295, device=cuda:0, global_loss=407, lr=[0.001], speed=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch06_acc_83.05191.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[8/30]: 100%|| 800/800 [01:23<00:00,  9.63it/s, accuracy=80.8, batch_loss=0.353, device=cuda:0, global_loss=357, lr=[0.001], speed=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch07_acc_80.83177.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[9/30]: 100%|| 800/800 [01:20<00:00,  9.89it/s, accuracy=86.1, batch_loss=0.179, device=cuda:0, global_loss=307, lr=[0.001], speed=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch08_acc_86.14759.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[10/30]: 100%|| 800/800 [01:21<00:00,  9.87it/s, accuracy=84.2, batch_loss=0.723, device=cuda:0, global_loss=285, lr=[0.001], speed=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch09_acc_84.24015.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[11/30]: 100%|| 800/800 [01:20<00:00,  9.96it/s, accuracy=82.7, batch_loss=0.351, device=cuda:0, global_loss=246, lr=[0.001], speed=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch10_acc_82.67667.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[12/30]: 100%|| 800/800 [01:21<00:00,  9.79it/s, accuracy=81.4, batch_loss=0.0644, device=cuda:0, global_loss=226, lr=[0.001], speed=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch11_acc_81.42589.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[13/30]: 100%|| 800/800 [01:21<00:00,  9.84it/s, accuracy=85.8, batch_loss=0.071, device=cuda:0, global_loss=205, lr=[0.001], speed=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch12_acc_85.83490.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[14/30]: 100%|| 800/800 [01:23<00:00,  9.56it/s, accuracy=84.1, batch_loss=0.0803, device=cuda:0, global_loss=203, lr=[0.001], speed=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch13_acc_84.11507.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[15/30]: 100%|| 800/800 [01:21<00:00,  9.82it/s, accuracy=84.2, batch_loss=0.0868, device=cuda:0, global_loss=195, lr=[0.001], speed=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch14_acc_84.17761.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[16/30]: 100%|| 800/800 [01:23<00:00,  9.55it/s, accuracy=84.4, batch_loss=0.0271, device=cuda:0, global_loss=167, lr=[0.001], speed=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch15_acc_84.42777.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[17/30]: 100%|| 800/800 [01:22<00:00,  9.71it/s, accuracy=85.3, batch_loss=0.0758, device=cuda:0, global_loss=155, lr=[0.001], speed=1.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch16_acc_85.33458.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[18/30]: 100%|| 800/800 [01:29<00:00,  8.90it/s, accuracy=85.6, batch_loss=0.0762, device=cuda:0, global_loss=144, lr=[0.001], speed=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch17_acc_85.58474.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[19/30]: 100%|| 800/800 [01:24<00:00,  9.43it/s, accuracy=85.5, batch_loss=0.0565, device=cuda:0, global_loss=138, lr=[0.001], speed=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch18_acc_85.52220.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[20/30]: 100%|| 800/800 [01:22<00:00,  9.66it/s, accuracy=86.1, batch_loss=0.313, device=cuda:0, global_loss=121, lr=[0.001], speed=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch19_acc_86.11632.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[21/30]: 100%|| 800/800 [01:21<00:00,  9.77it/s, accuracy=83.8, batch_loss=0.143, device=cuda:0, global_loss=116, lr=[0.001], speed=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch20_acc_83.80238.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[22/30]: 100%|| 800/800 [01:20<00:00, 10.00it/s, accuracy=83.6, batch_loss=0.109, device=cuda:0, global_loss=118, lr=[0.001], speed=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch21_acc_83.55222.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[23/30]: 100%|| 800/800 [01:44<00:00,  7.66it/s, accuracy=85.3, batch_loss=0.0856, device=cuda:0, global_loss=104, lr=[0.001], speed=0.968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch22_acc_85.33458.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[24/30]: 100%|| 800/800 [01:24<00:00,  9.48it/s, accuracy=84.9, batch_loss=0.0134, device=cuda:0, global_loss=97.4, lr=[0.001], speed=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch23_acc_84.89681.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[25/30]: 100%|| 800/800 [01:27<00:00,  9.12it/s, accuracy=84.8, batch_loss=0.00415, device=cuda:0, global_loss=96.4, lr=[0.001], speed=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch24_acc_84.77173.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[26/30]: 100%|| 800/800 [01:25<00:00,  9.35it/s, accuracy=85.6, batch_loss=0.0159, device=cuda:0, global_loss=83.5, lr=[0.001], speed=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch25_acc_85.64728.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[27/30]: 100%|| 800/800 [01:23<00:00,  9.54it/s, accuracy=84.2, batch_loss=0.092, device=cuda:0, global_loss=80.7, lr=[0.001], speed=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch26_acc_84.24015.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[28/30]: 100%|| 800/800 [01:15<00:00, 10.66it/s, accuracy=84.7, batch_loss=0.0233, device=cuda:0, global_loss=75.6, lr=[0.001], speed=1.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch27_acc_84.74046.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[29/30]: 100%|| 800/800 [01:11<00:00, 11.17it/s, accuracy=85.4, batch_loss=0.00856, device=cuda:0, global_loss=84.4, lr=[0.001], speed=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch28_acc_85.39712.pth.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[30/30]: 100%|| 800/800 [01:00<00:00, 13.20it/s, accuracy=85.9, batch_loss=0.416, device=cuda:0, global_loss=97.1, lr=[0.001], speed=1.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint: /home/ec2-user/SageMaker/checkpoints/uncropped/checkpoint_Model_v2_epoch29_acc_85.92871.pth.tar ...\n",
      "Train Accuracy: 99.70289288506646\n",
      "Validation Accuracy: 85.92870544090057\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer._train_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d735d-e88e-49c1-ad63-9bee895ea005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# hyperparameter_grid = {\n",
    "#     'model': [Model_v2], # adding more layers # adding more maxpool and make \n",
    "#     'lr': [0.001], # imp\n",
    "#     'weight_decay': [1e-7],\n",
    "#     'dropout': [0.2], #imp\n",
    "#     'loss_weight': [0.3,0.5,1.0,1.5,2.0,5.0],\n",
    "#     'batch_size': [16] # most imp\n",
    "# }\n",
    "\n",
    "# # Create a list of all possible combinations of hyperparameters\n",
    "# hyperparameter_combinations = list(itertools.product(*hyperparameter_grid.values()))\n",
    "\n",
    "# # Define a function for training and evaluating the model with a given set of hyperparameters\n",
    "# def train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size):\n",
    "#     # Create the model\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model_class(dropout)\n",
    "#     model.to(device)\n",
    "#     parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    \n",
    "#     # Create the optimizer\n",
    "#     optimizer = optim.Adam(\n",
    "#         params=parameters,\n",
    "#         lr=lr,\n",
    "#         betas=(0.8, 0.999),\n",
    "#         eps=1e-8,\n",
    "#         weight_decay=weight_decay)\n",
    "#     tensorboard_dir = '/home/ec2-user/SageMaker/runs/'\n",
    "#     # Create the scheduler\n",
    "#     lr_warm_up_num = 100\n",
    "#     cr = 1.0 / math.log(lr_warm_up_num)\n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(\n",
    "#         optimizer,\n",
    "#         lr_lambda=lambda ee: cr * math.log(ee + 1)\n",
    "#         if ee < lr_warm_up_num else 1)\n",
    "    \n",
    "#     # Create the loss criterion\n",
    "#     data = pd.read_csv('dataset/uncropped/annotations.csv')\n",
    "#     label_counts = data.iloc[:, 1].value_counts()\n",
    "#     loss_ratio = label_counts[0] / label_counts[1]\n",
    "#     weight = torch.Tensor([1,loss_ratio*loss_weight]).to(device)\n",
    "#     loss_criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    \n",
    "#     # Load the data with the specified batch size\n",
    "#     dataset_length = len(dataset)\n",
    "#     train_size = int(0.7 * dataset_length)\n",
    "#     test_size = dataset_length - train_size\n",
    "#     train_set, test_set = torch.utils.data.random_split(dataset, [train_size,test_size])\n",
    "\n",
    "#     train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "#     # Create the Trainer object\n",
    "#     trainer = Trainer(\n",
    "#         model, loss_criterion,\n",
    "#         train_data_loader=train_loader,\n",
    "#         test_data_loader=test_loader,\n",
    "#         optimizer=optimizer,\n",
    "#         epochs=30,\n",
    "#         with_cuda=True,\n",
    "#         use_scheduler=True,\n",
    "#         scheduler=scheduler,\n",
    "#         save_dir='/home/ec2-user/SageMaker/checkpoints/uncropped/',\n",
    "#         tensorboard_dir=tensorboard_dir,\n",
    "#         lr=lr)\n",
    "    \n",
    "#     # Train the model\n",
    "#     trainer._train_epoch()\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     accuracy = trainer._valid_epoch(model, 0, test_loader)\n",
    "    \n",
    "#     return accuracy\n",
    "\n",
    "# # Perform grid search\n",
    "# best_accuracy = 0.0\n",
    "# best_hyperparameters = None\n",
    "\n",
    "# for hyperparameters in hyperparameter_combinations:\n",
    "#     model_class, lr, weight_decay, dropout, loss_weight, batch_size = hyperparameters\n",
    "#     print(f\"Training model {model_class.__name__} with lr={lr}, weight_decay={weight_decay}, dropout={dropout}, loss_weight={loss_weight}, batch_size={batch_size}...\")\n",
    "#     accuracy = train_and_evaluate_model(model_class, lr, weight_decay, dropout, loss_weight, batch_size)\n",
    "#     print(f\"Accuracy: {accuracy}%\")\n",
    "    \n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_hyperparameters = hyperparameters\n",
    "        \n",
    "# print(best_accuracy)\n",
    "# print(best_hyperparameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
